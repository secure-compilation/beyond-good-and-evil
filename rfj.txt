================================================================================
| Names for class and object declarations
================================================================================

In the language definition we will use the notion of names. Names will
be considered to be naturals. When compiling to a realistic
architecture, we will require that these naturals are word-sized.

CH: Integers or naturals? I think natural should do.

YJ: Yes, that's what I meant by integers actually, my mistake.
    Updated everywhere.

--------------------------------------------------------------------------------
| Q: Could we get rid of names?
--------------------------------------------------------------------------------

A:

When looking at a *complete* program (i.e. a program that provides all
the definitions it relies on), it looks like names are useless. One
could just use the order of definitions instead of names, to get an
index for each declaration, and use that index everytime we refer to
the corresponding declaration.

When we consider *partial* programs, however, names are very
handy. They allow us to have a very straightforward definition of the
linking of two partial programs.

Here, we want to consider partial programs for formal reasons: it
allows us to have a clear notion of *context* (or *attacker*). Indeed,
we want a program to be able to refer to attacker definitions, and the
other way (a context to refer to program definitions). Names provide
an easy solution to this problem. They allow us to have a symmetry
between programs and contexts, and to consider mutually distrustful
programs, each being part of the context for the others.


--------------------------------------------------------------------------------
| Q: Could we have e.g. strings for names?
--------------------------------------------------------------------------------

A:

One could get a user-friendly variant of our language by using strings
for names. Then we could easily translate programs by using e.g. :

* a hash algorithm to map these strings to word-sized naturals, asking
the user to give new names in case of collision, this approach would
be interesting if we want to translate partial programs (or contexts),
which would allow reusing the linking defined in the language with
naturals as names ;

* the order of declarations, this approach would be interesting if we
only care about translating complete programs, that is, it requires to
define a new notion of linking at the level of the language with
strings if we want to talk about partial programs (or contexts) at
this level.

In both cases the main object (which would presumably have name
"main") should be a special case which is necessarily mapped to
natural 0 (the only fixed name in the language with naturals as
names).

The properties one would get on such a variant of our
language would be the same as the ones we will describe (maybe with
some extra assumptions to prevent name clashes). From now on, we go
back to the naturals as names variant.

================================================================================
| Syntax
================================================================================

C - class name -- mapped to class declarations L (CT)
f - field index (natural)
m - method index (natural)
o - object names -- mapped to object declarations O (OT)

P ::= (CT, OT)                   (partial) program

CT ::= partial map from C to L   class table

OT ::= partial map from o to O   object table

L ::=                            class declaration
  class {
    C1, ..., Cn;                   field declarations
    M1, ..., Mp                    method declarations
  }

  where n, p >= 0

O ::=                            object declaration
  obj C { o1, ..., on }

  where n >= 0

M ::=                            method declaration
  Cr (Ca) {                        result and arg type
    e                              method body
  }

x ::=
  | this
  | arg

e ::=                            expressions
  | x                              variables
  | o                              object reference
  | e.f                            field selection
  | e.m(e)                         method call
  | e == e ? e : e                 object identity test


--------------------------------------------------------------------------------
| Q: Could we have a real syntax? Partial maps are not syntax.
--------------------------------------------------------------------------------

A:

Sure.

Q ::= [] | D :: Q                (partial) program

D ::=                            declaration
  | C => L                         named class declaration
  | o => O                         named object declaration

This definition trivially translates to the previous definition. The only
tricky point is name clashes, which we do not want to care about
here. That's why we will use the definition with partial maps from now
on.

We could also have a more Java-like syntax that would very simply
translate to the previous one.

================================================================================
| Operational semantics
================================================================================

--------------------------------------------------------------------------------
| Def.: Reduction relation on expressions.
--------------------------------------------------------------------------------

OT(o) = obj C { o1, ..., on }
f = i in [1,n]
----------------------------- SEL
(CT, OT) |- o.f --> oi


OT(o) = obj C { o1, ..., on }
CT(C) = class {...; M1, ..., Mp }
Mj = Cr (Ca) { e }
m = j in [1,p]
--------------------------------------- CALL
(CT, OT) |- o.m(oa) --> e[oa/arg][o/this]


------------------------------- TESTEQ
(CT, OT) |- o == o ? e : e' --> e

o <> o'
--------------------------------- TESTNEQ
(CT, OT) |- o == o' ? e : e' --> e'

e1 --> e1'
---------------------------------- CONGRLCALL
(CT, OT) |- e1.mi(e2) --> e1'.mi(e2)


(CT, OT) |- e --> e'
------------------------------ CONGRRCALL
(CT, OT) |- o.mi(e) --> o.mi(e')

(CT, OT) |- e1 --> e1'
---------------------------------------------------- CONGRLTEST
(CT, OT) |- e1 == e2 ? e3 : e4 --> e1' == e2 ? e3 : e4

(CT, OT) |- e2 --> e2'
-------------------------------------------------- CONGRRTEST
(CT, OT) |- o == e2 ? e3 : e4 --> o == e2' ? e3 : e4


We sometimes use the notation e --> e', where CT and OT are implicitly assumed.

--------------------------------------------------------------------------------
| Def.: Values
--------------------------------------------------------------------------------

A value is an object name.


--------------------------------------------------------------------------------
| Prop.: Values do not reduce
--------------------------------------------------------------------------------
| forall (v:value) (e: expr), v -/-> e.
--------------------------------------------------------------------------------

Proof: No rule applies when we have an object name in the left hand side.


--------------------------------------------------------------------------------
| Prop.: Equality on values is decidable
--------------------------------------------------------------------------------
| forall v v', {v = v'} + {v <> v'}.
--------------------------------------------------------------------------------

Proof: Object names are naturals.

--------------------------------------------------------------------------------
| Prop.: Determinism of our reduction relation
--------------------------------------------------------------------------------
| forall e e', e --> e'  ->  (forall e'', e --> e''  ->  e' = e'')
--------------------------------------------------------------------------------

Proof: Corollary of the fact that values (hence object names) do not
reduce.

By induction on the size of the derivation tree for e --> e'. By case
on the last rule applied in this tree. Only this rule applies with e
as a the left-hand side, because object names do not reduce and
equality on values is decidable. This gives the shape of e'', and is
sufficient to conclude in the base case. In the inductive case,
applying the induction hypothesis ends the proof.

CH: sketchy, in Coq a simpler proof is to build an interpreter and
    prove it correct (that interpreter is then useful on its own right)

--------------------------------------------------------------------------------
| Def.: Evaluation
--------------------------------------------------------------------------------

Given a program P = (CT, OT), evaluation starts with the expression:

                 o.m(o)     with o = 0 and m = 0

Hence, only programs that define object 0 with a class C such that C
has at least one method will have a non-stuck initial expression. This
will be part of the notion of complete programs in our type system.


================================================================================
| Type system
================================================================================

We give a type system that allows to typecheck *complete* programs. A
program that depends on classes (resp. objects) that are not in the
domain of CT (resp. OT) will *not* typecheck. Similarly, programs that
do not define a main object will *not* typecheck.

--------------------------------------------------------------------------------
| Def.: Class of an object
--------------------------------------------------------------------------------

  classof(o) = C   if OT(o) = obj C {...}

--------------------------------------------------------------------------------
| Def.: Object declaration typing
--------------------------------------------------------------------------------

OT(o) = obj C {o1, ..., on}
CT(C) = class {C1, ..., Cn; ...}
forall i in [1,n]. classof(oi) = Ci
----------------------------------- OBJOK
(CT, OT) |- o ok

--------------------------------------------------------------------------------
| Def.: Expression typing
--------------------------------------------------------------------------------

We define typing environments as:

                           G ::= {this:Cr, arg:Ca}


----------------------- TVAR
(CT, OT); G |- x : G(x)


----------------------------- TOBJ
(CT, OT); G |- o : classof(o)


(CT, OT); G |- e : C
G(this) = C                                   ensures field privacy
CT(C) = class {C1, ..., Cn; ...}
f = i in [1,n]
-------------------------------- TSEL
(CT, OT); G |- e.f : Ci


(CT, OT); G |- e : C
CT(C) = class {...; M1, ..., Mp}
m = j in [1,p]
Mj = Cr (Ca) { ... }
(CT, OT); G |- ea : Ca
-------------------------------- TCALL
(CT, OT); G |- e.m(ea) : Cr


(CT, OT); G |- e1 : C'
(CT, OT); G |- e2 : C'
(CT, OT); G |- e3 : C
(CT, OT); G |- e4 : C
--------------------------------------- TTEST
(CT, OT); G |- (e1 == e2 ? e3 : e4) : C


--------------------------------------------------------------------------------
| Def.: Method typing
--------------------------------------------------------------------------------

(CT, OT); {this:C, arg:Ca} |- e : Cr
------------------------------------ METHOK
(CT, OT); C |- Cr (Ca) { e } ok


--------------------------------------------------------------------------------
| Def.: Class typing
--------------------------------------------------------------------------------

CT(C) = class {C1, ..., Cn; M1, ..., Mp}
forall i in [1,n]. Ci in dom(CT)
forall j in [1,p]. (CT, OT); C |- Mj ok
---------------------------------------- CLASSOK
(CT, OT) |- C ok

CH: Should make sure that one can build finite derivations using this
    even when classes are mutually dependent. I'm a bit worried actually.

--------------------------------------------------------------------------------
| Def.: (Complete) program typing
--------------------------------------------------------------------------------

P = (CT, OT)
forall C' in dom(CT). (CT, OT) |- C' ok
forall o in dom(OT). (CT, OT) |- o ok
let o = 0 and m = 0 in
  (CT, OT); {this: -, arg: -} |- o.m(o) : C             main object
------------------------------------------- TPROG
|- P : C

|- P : C
-------- PROGOK
|- P ok

More explicit, equivalent alternative:

P = (CT, OT)
forall C' in dom(CT). (CT, OT) |- C' ok
forall o in dom(OT). (CT, OT) |- o ok
OT(0) = obj C'' { ... }
CT(C'') = { ...; M1, ..., Mp }
p >= 1
M1 = C (C'') { e }
--------------------------------------- TPROG'
|- P : C


--------------------------------------------------------------------------------
| Q: Should we also have partial programs typing?
--------------------------------------------------------------------------------

If we want to state full abstraction for (partial) programs, it goes
by:
                 P ~ Q  <->  compile(P) ~ compile(Q)

So we would need to define at least compilation for partial
programs. But we don't necessarily need partial program typing,
although it could be interesting to study. See our notion of
observational equivalence below to understand why we don't need it.


================================================================================
| Encodings
================================================================================

--------------------------------------------------------------------------------
| Tuples
--------------------------------------------------------------------------------

For each tuple type T=(C1, ..., Cn)

class Unit {}

class Tuple_T {
  C1, ..., Cn;
  C1 get_1(Unit) { this.1 },
  ...
  Cn get_n(Unit) { this.n },
}

--------------------------------------------------------------------------------
| Booleans
--------------------------------------------------------------------------------

class Bool {}

obj true : Bool {}
obj false : Bool {}

If then else via identity test (e.g. (b == true ? e1 : e2)).


================================================================================
| Properties of our type system
================================================================================

(To be proved)

--------------------------------------------------------------------------------
| Progress
--------------------------------------------------------------------------------
| forall e C P G, |- P ok ->
|                 P; G |- e : C ->
|                   is_value e \/ exists e', P |- e --> e'
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
| Preservation (Subject reduction)
--------------------------------------------------------------------------------
| forall e e' C P G, P; G |- e : C ->
|                    P |- e --> e' ->
|                      P; G |- e' : C
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
| Type safety
--------------------------------------------------------------------------------
| forall e C P G, P; G |- e : C ->
|                      is_value e \/ (exists e', e --> e' /\ |- e' : C)
--------------------------------------------------------------------------------


================================================================================
| High-level programs, high-level contexts
================================================================================

--------------------------------------------------------------------------------
| Def.: Linkability
--------------------------------------------------------------------------------

Two (partial) programs P = (CT, OT) and Q = (CT', OT') are linkable if
their domains are disjoint, that is:

  linkable (CT, OT) (CT', OT') ::=
    dom(CT) inter dom(CT') = empty_set /\
      dom(OT) inter dom(OT') = empty_set


--------------------------------------------------------------------------------
| Def.: Linking
--------------------------------------------------------------------------------

Two linkable (partial) programs P = (CT, OT) and Q = (CT', OT') can be
linked to get a new (partial) program P /\/\ Q.

     P /\/\ Q ::= (CT disjoint_union CT', OT disjoint_union OT')

Note that linking happens "before" typing: we do not require any
wellformedness property on P and Q.


--------------------------------------------------------------------------------
| Def.: High-level context
--------------------------------------------------------------------------------

A high-level context c is a partial program. A program can only be
inserted in a linkable context. Inserting a program P in a context c
is linking:

     c[P] ::= P /\/\ c       when linkable c P

--------------------------------------------------------------------------------
| Def.: Closing contexts
--------------------------------------------------------------------------------

Given a program P, a context c, a class name C, we say that c closes P
with type C when c is linkable with P and inserting P in c gives a
well-typed program of type C, that is:

                  linkable c P  /\  |- c[P] : C

We define the set of closing contexts at type C for a program P by:

     closing_contexts P C ::= {c | linkable c P  /\  |- c[P] : C}

CH: TODO: These seem both syntactically broken.
    Programs are not given types, they are not expressions.

YJ: In the first approach, we had a main expression that was distinct
    from the program.
    I have changed things so that now, their is no main expression,
    but a main object which is part of the program: object 0. Then,
    evaluation begins by calling the first method of this object.
    In the latest set of rules (see TPROG, TPROG'), programs are given
    a type which would be the one of their "result value", that is,
    the result type of calling the first method on the first object.
    Is this bad practice?

--------------------------------------------------------------------------------
| Def.: Interface equivalence
--------------------------------------------------------------------------------

We say that two programs P and Q are interface equivalent, or have the
same interface, when they have the same closing contexts at every
type, that is:

  same_interface P Q := forall C. closing_contexts P C = closing_contexts Q C


--------------------------------------------------------------------------------
| Def.: Observational equivalence
--------------------------------------------------------------------------------

Two programs P and Q are observationnally equivalent if they are
interface equivalent and, for any context c that closes them, when
separately placing the two in c, the two resulting programs reduce to
the same value. That is:

  P ~ Q ::= same_iterface P Q /\
            forall C. forall c in closing_contexts P C. forall v.
            let o = 0 and m = 0 in
            c[P] |- o.m(o) -->* v  <->  c[Q] |- o.m(o) -->* v


--------------------------------------------------------------------------------
| Def.: Observational order
--------------------------------------------------------------------------------

We define the observational order on programs as follows:

  P ~< Q ::=
    forall C. forall c in closing_contexts P C.
    c in closing_contexts Q C /\ forall v.
                                 let o = 0 and m = 0 in
                                 c[P] |- o.m(o) -->* v  ->
                                   c[Q] |- o.m(o) -->* v

When we have P ~< Q, this means that Q captures all the behaviors of P.

CH: Missing same_interface here?

YJ: For this definition I only need one way of same_interface, that is:
      forall C. forall c in closing_contexts P C.
        c in closing_contexts Q C.
    It could indeed be obtained directly from same_interface, and I
    don't know what definition would be the most interesting. I'm not
    sure how much this order is useful anyway. It looks a bit
    artificial, but some people seem to like it (e.g. in the Java
    Jr. paper, contextual equivalence is defined in terms of such an
    order).
    We can think more about what definition looks more relevant when
    we start to use ~< (if we ever use it).

--------------------------------------------------------------------------------
| Prop.: Decomposition of observational equivalence
--------------------------------------------------------------------------------
| forall P Q. P ~ Q <-> (P ~< Q and Q ~< P)
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
| Q: Is our notion of observational equivalence relevant?
--------------------------------------------------------------------------------

This definition considers that only the final result of a program is
observable. This looks reasonable as long as *we do not have side
effects* (in particular: printing). Every time we update the source
language, we should check very carefully if this definition is still
relevant or should be updated.

CH: There are other sanity checks we could try; one can for instance
    prove this observational equivalence equivalent with other
    definitions, e.g. observing only termination behavior

YJ: Good idea.

================================================================================
| "Really abstract" machine
================================================================================

Here we present an abstract machine. The goal is to capture exactly the
properties of the source language, while offering a lower-level view. In
particular, we present a compilation scheme to this abstract machine
for which full abstraction should be easy to show.
Here full abstraction should be unconditional ; in
particular there is no notion of resource exhaustion: execution works
with infinite stacks.

The hope is that this lower-level view will help when moving to other
machines. Actually, the other machines that we will present may be
seen as implementations of this one.


--------------------------------------------------------------------------------
| Syntax
--------------------------------------------------------------------------------

 c ::= n                    class index (where n is a natural number)
 o ::= (c, n)               object index (where n is a natural number)
 f ::= n                    field index (where n is a natural number)
 m ::= n                    method index (where n is a natural number)

 instr ::=
 | Nop
 | This                     copy from this to res
 | Arg                      copy from arg to res
 | Push                     push res to current compartment's "user stack"
 | Ref o                    load constant o to res
 | Select f                 select a field of the object in res
 | Call m                   call a method of the object on top of the stack
                            (which gets removed from the stack in the process)
                            (current result is used as argument)
 | Beq n                    equality test with forward jump
                            (skip n instructions if equal)
                            (compares res with top value on the stack)
                            (the stack top value gets removed in the process)


CH: Descriptions above are not understandable without some more
    high-level intuition. Display all this in reverse order.

 code ::= [] | instr :: code

 methods ::= [] | code :: methods
 fields ::= [] | o :: fields

 class = methods
 object = fields

 objects ::= [] | object :: objects

 user_stack ::= [] | o :: user_stack

 comp ::= (class, objects, user_stack)
 comps ::= [] | comp :: comps

 reg ::= (o, o, o)

    used as a mapping from {this, arg, res} to object indices (o)
    Notations: (o,_,_)[this]=o ; (_,o,_)[arg]=o ; (_,_,o)[res]=o

 call_stack ::= [] | (o, o, code) :: call_stack

    at call time, we store the current object and argument
    together with a continuation

 state ::= (compartments, reg, call_stack, code)

At this level we make no distinction between inter- and intra-
compartment calls. If we were to need this distinction it would anyway
be lower level.

CH: This abstract machine is fine for now, but bakes in an inefficient
    compilation strategy with only 3 registers in use and lots of
    stack fiddling. At some point we might think about compilation
    schemes that use all the registers.

--------------------------------------------------------------------------------
| Def.: Reduction
--------------------------------------------------------------------------------

A common (implicit) premise to _all_ rules below is:
  reg[this] = (c, n)
  comps[c] = (_, objects, _)
  n <= |objects|


Now the rules:

  -----------------------------------------------------------------------
  (comps, reg, call_stack, Nop :: code) -> (comps, reg, call_stack, code)

  reg[res] = o
  reg[this] = (c, _)
  comps[c] = (class, objects, user_stack)
  comps' = comps[c -> (class, objects, o :: user_stack)]
  -------------------------------------------------------------------------
  (comps, reg, call_stack, Push :: code) -> (comps', reg, call_stack, code)

  o = (co, no)
  comps[co] = (_, objects, _)                 } what if we do
  no <= |objects|                             } not check this?
            (probably: harder proofs but nothing bad happens?
             actually: would break full abstraction?
               if this is attacker code, then instead of failing here we would
               fail only when this object becomes "this", so this could allow
               to learn if a given class performs calls on a given argument
               or not)
  reg' = reg[res <- o]
  --------------------------------------------------------------------------
  (comps, reg, call_stack, Ref o :: code) -> (comps, reg', call_stack, code)

  reg[this] = o
  reg' = reg[res <- o]
  -------------------------------------------------------------------------
  (comps, reg, call_stack, This :: code) -> (comps, reg', call_stack, code)

  reg[arg] = o
  reg' = reg[res <- o]
  ------------------------------------------------------------------------
  (comps, reg, call_stack, Arg :: code) -> (comps, reg', call_stack, code)

  reg[res] = (co, no)
  reg[this] = (co, _)       (same compartment side condition; private fields)
  comps[co] = (_, objects, _)
  objects[no] = fields
  fields[f] = of
  reg' = reg[res <- of]
  -----------------------------------------------------------------------------
  (comps, reg, call_stack, Select f :: code) -> (comps, reg', call_stack, code)

  reg[this] = othis
  reg[arg] = oarg
  othis = (c, _)
  comps[c] = (class, objects, o :: s)
  comps' = comps[c -> (class, objects, s)]
  o = (co, no)
  comps[co] = (class, _, _)
  class[m] = code'
  reg[res] = oa
  reg' = reg[this <- o, arg <- oa,
             res <- o]    fix a default return value (the object itself)
                          / overwrite caller stuff
  ---------------------------------------------------------------------
  (comps, reg, call_stack, Call m :: code) ->
    (comps', reg', (othis, oarg, code) :: call_stack, code')

                                                     return from call
  reg' = reg[this <- othis, arg <- oarg]
  -------------------------------------------------------------------
  (comps, reg, (othis, oarg, code) :: call_stack, []) ->
                                  (comps, reg', call_stack, code)

  reg[this] = (c, _)
  comps[c] = (class, objects, o :: s)
  comps' = comps[c <- (class, objects, s)]
  reg[res] = o  
  code' = drop n code
  --------------------------------------------------
  (comps, reg, Beq n :: code) -> (comps, reg, code')

  reg[this] = (c, _)
  comps[c] = (class, objects, o :: s)
  comps' = comps[c <- (class, objects, s)]
  reg[res] = or
  o <> or
  -------------------------------------------------
  (comps, reg, Beq n :: code) -> (comps, reg, code)


--------------------------------------------------------------------------------
| Def.: Compilation of expressions
--------------------------------------------------------------------------------

Define:

  class_id(CT, C) = n
    if C in dom(CT)
    and n = |{C' in dom(CT) | C' < C}|

  obj_id(CT, OT, o) = (n, m)
    if OT(o) = obj C { ... }
    and n = class_id(CT, C)
    and m = |{o' in dom(OT) | OT(o) = obj C { ... } and o' < o}|

c_expr(CT, OT, this) = This
  (G(this) is defined in this case)

c_expr(CT, OT, arg) = Arg
  (G(arg) is defined in this case)

c_expr(CT, OT, o) = Ref obj_id(CT, OT, o)

c_expr(CT, OT, e.f) =
     c_expr(CT, OT, e);
     Select f

c_expr(CT, OT, e.m(e')) =
     c_expr(CT, OT, e);
     Push;
     c_expr(CT, OT, e');
     Call m

c_expr(CT, OT, e1 == e2 ? e3 : e4) =
     c_expr(CT, OT, e1);
     Push;
     c_expr(CT, OT, e2);
     Beq EQ;       (note: these are indeed relative forward jumps, so this works)
     c_expr(CT, OT, e4);
     Push;         }  <--- a way to encode unconditional forward jump
     Beq END;      }
EQ:  c_expr(CT, OT, e3);
END: Nop


--------------------------------------------------------------------------------
| Def.: Compilation of (complete) programs
--------------------------------------------------------------------------------

Compiling a complete program is quite straightforward once we know how
to compile expressions. We can get a list of class out of CT by
skipping "holes" in the domain, and a list of objects for each class
by skipping "holes" in the domain and using one counter per class.


--------------------------------------------------------------------------------
| Problems & questions
--------------------------------------------------------------------------------

* We only define a compilation for complete programs. How can we state
full abstraction in such a setting?

                   P ~ Q  <->  compile P ~ compile Q

At the high level, we said that P and Q might be partial, so we should
have a compilation for partial programs, and a notion of observational
equivalence for the result of compiling high-level partial programs (=
low-level partial programs?).

* Can we still have something as simple as lists of classes &
per-compartment lists of objects? In particular, our definition for
obj_id will not be usable in this setting. Should we update the source
language to have object indices represented as (C, n) at the source
level already? Would this help?

----> Maybe we should think about what happens on more concrete
      machines wrt compiling partial programs, before trying to answer
      these questions.

If we have partial programs and linking at the low level, then linking
should be associative at both levels, and an interesting property of
the compilation scheme would be:

 compile (P1 /\/\ ... /\/\ Pn) = compile P1 /\/\ ... /\/\ compile Pn

================================================================================
| Symbolic micro-policy (compartmentalization variant)
================================================================================

Many differences with respect to Oakland 2015 policy:
  - Simplifications:
    - static compartments, no way to split things
    - no jump and write targets
      (just a way for all compartments to call all monitor services)
  - Extensions:
    - prohibiting all intra-compartment reads
    - protected call stack in monitor space
    - monitor services for call, return, and select
  - Differences:
    - monitor services that return in a different compartment (call and return)

--------------------------------------------------------------------------------
| Symbolic tags
--------------------------------------------------------------------------------

Tm  ::= c | _|_         once we have allocation, _|_ would be unallocated space
Tr  ::= ()
Tpc ::= c            Q: do we need to switch to <F,c> where F in {Jal,NoJal}?
Ts  ::= ()

--------------------------------------------------------------------------------
| Symbolic rules
--------------------------------------------------------------------------------

Nop: {tpc=c, tci=c} -> {tpc'=c}

Const: {tpc=c, tci=c} -> {tpc'=c, trd'=()}

Mov: {tpc=c, tci=c, trs=(), trd=()} -> {tpc'=c, trd'=()}

Binop: {tpc=c, tci=c, tr1=(), tr2=()} -> {tpc'=c, trd'=()}

Load: {tpc=c, tci=c, trp=(), tm=c, trd=()} -> {tpc'=c, trd'=()}

Store: {tpc=c, tci=c, trp=(), trs=(), tm=c} -> {tpc'=c, tm'=c}

Jump: {tpc=c, tci=c, tr=()} -> {tpc'=c}

Jal: {tpc=c, tci=c, tr=(), tra=()} -> {tpc'=c, tra=()}

Bnz: {tpc=c, tci=c, tr=()} -> {tpc'=c}

Service: {tpc=c, tci=()} -> {}

--------------------------------------------------------------------------------
| Extra state
--------------------------------------------------------------------------------

call_stack : list (word, word, word)

lookup_method : obj -> nat -> option addr

lookup_field : obj -> nat -> option addr

--------------------------------------------------------------------------------
| Monitor services
--------------------------------------------------------------------------------

call(mem, reg, pc@c, (call_stack, lookup_method, lookup_field)) =
  let o = reg[r_arg1] in
  let arg = reg[r_arg2] in
  let m = reg[r_arg3] in
  ma <- lookup_method o m;
  if tag mem[reg[ra]] = c then
    let call_stack' = (reg[ra], reg[r_this], reg[r_arg]) :: call_stack in
    let reg' = (clear_registers reg)[r_this := o, r_arg := arg] in
    let c' = mem[ma] in
    Some (mem, reg', ma@c', (call_stack', lookup_method, lookup_field))
  else None

return(mem, reg, pc@c, (call_stack, lookup_method, lookup_field)) =
  match call_stack with
  | (ret, this, arg) :: call_stack' ->
    let reg' = (clear_registers reg)[r_ret := reg[r_ret],
                                     r_this := this, r_arg := arg] in
    let c' = mem[ret] in
    Some (mem, reg', ret@c', (call_stack', lookup_method, lookup_field))
  | [] -> None

  is this useful??
    |   if we remove it, then lookup_field shouldn't be extra state,
    |   we should have a fixed scheme instead
    v

select(mem, reg, pc@c, (call_stack, lookup_method, lookup_field)) =
  let o = reg[r_arg1] in
  let f = reg[r_arg2] in
  fa <- lookup_field o f;
  if tag mem[fa] = c then      <---- private fields
    let reg' = reg[r_ret := (val mem[fa])@()] in
    Some (mem, reg', ra@c, (call_stack, lookup_method, lookup_field))
  else
    None

Note: allowing returns to forged ra addresses seem ok because:
- for call we enforce that ra points to the caller's compartment
  - alternatively, we could store the caller's compartment on the call
    stack and restore it on return
- for select we preserve the same compartment id, so if the original
  value of ra was outside the compartment, the machine will stop on
  the next instruction fetch
- generally, our impression is that a return to an arbitrary place in
  the malicious component that invoked the service doesn't give the
  attacker additional power (the malicious component can already jump
  to any of its addresses); making this intuition formal in the full
  abstraction proof might be hard (or impossible if the intuition
  happens to be wrong), in which case we should require that monitor
  services are only reached via Jal


================================================================================
| Compilation scheme (old)
================================================================================

Define two tables:

  CIT (Class Index Table): maps classes to a number
  OIT (Object Index Table): maps objects to a number

And for each class:

  fieldoffset(C, f)
  methodoffset(C, m)

--------------------------------------------------------------------------------

Monitor services in the abstract machine:

* the idea: one stack per component ;
* two design choices:
    - a global stack handled by the monitor ;
    - or each stack located within the compartment of the component ;
* here: abstract machine, so we can manage them in the state for the moment.

/!\ need to forbid Jump-ing between compartements (e.g. for calling monitor services)
    --> only Jal
    (otherwise, we don't know who did the call, r_a may have been maliciously set by the attacker
     e.g. the stack of a component could be compromised by calling PUSH)

reg[r_arg1] = w
reg[r_a] = pc'
pc' in C_i
pc' - 1 in C_i                                   forbid jumps between distinct compartments
-----------------------------------------------------------------------------------------------
(mem, reg, push_addr, s_1, ..., s_i, ..., s_n) -> (mem, reg, pc', s_1, ..., w :: s_i, ..., s_n)

pc' in C_i
pc' - 1 in C_i                                   forbid jumps between distinct compartments
reg' = reg[r_ret <- w]
----------------------------------------------------------------------------------------------
(mem, reg, pop_addr, s_1, ..., w :: s_i, ..., s_n) -> (mem, reg', pc', s1, ..., s_i, ..., s_n)

mem[pc] = i
decode i = CALL rc rm ro rp
reg[rc] = c
reg[rm] = m
reg[ro] = o
reg[rp] = p

----------------------------
(mem, reg, pc, s) -> (mem, reg', pc', s)

mem[pc] = i
decode i = 
??
---------------
??

mem[pc] = i
decode[i] = Get ro rd
??
-----------------------
??

--------------------------------------------------------------------------------

Compilation of well-typed expressions:
  Variant #1: - doing everything via system calls
              - protection via compartmentalization micro-policy

Fix four distinct registers: r_this, r_arg, r_arg1, r_arg2, r_arg3, r_res (=r_ret), r_aux.

G = {this:Cr, arg:Ca} when compiling a method body.
G = {} when compiling the main expression.

Properties of c_expr:
- invariant: r_arg and r_this are preserved
            (they can change internally, but are restored at the end)
- can overwrite all other registers
- writes result to r_res
- finishes with same stack as it was called (restores the original stack)

c_expr(this) = Mov r_this r_res
  (G(this) is defined in this case)

c_expr(arg) = Mov r_arg r_res
  (G(arg) is defined in this case)

c_expr(o) = Const o r_res

c_expr(e.f) =
    c_expr(e);
    Mov r_res r_arg1;
    Cons i_f r_arg2;
    Jal SELECT
  if G |- e : C (deterministic)
  and i_f = fieldoffset(C, f)

c_expr(e.m(e')) =
    c_expr(e);
    Jal PUSH r_res;
    c_expr(e');
    Pop r_arg1;
    Mov r_res r_arg2;
    Const i_m r_arg3;
    Push r_this;
    Push r_arg;
    Jal CALL
    Pop r_arg;
    Pop r_this
  if G |- e : C (deterministic)
  and i_m = methodoffset(C, m)

c_expr(e1 == e2 ? e3 : e4) =
    c_expr(e1);
    Push r_res;
    c_expr(e2);
    Pop r_aux;
    Binop_= r_aux r_res r_aux;
    Bnz r_aux LST;
    c_expr(e4);
    Jump END;
LST:c_expr(e3);
END:Nop

Q: How to represent the auxiliary stack of saved registers?
- globally, accessed via system calls
  - could be merged with global call stack:
    - stack frames between return addresses
    - Pop can't affect return addresses, only Return can
- locally, each compartment keeps its own stack
  - would work well with all alts below

--------------------------------------------------------------------------------

  Variant #2: - everything directly via instructions
                - monitor call only for index_to_addr (later object allocation)
              - fine-grained micro-policy
              - TODO: still unclear what to do with the stack(s)
                - alt1: keep using monitor service for call stack
                - alt2: protected call stack outside the monitor
                        (micro-policy for call-stack protection)
                - alt3: lightweight return addr protection + only local stacks

c_expr(e.f) =
    c_expr(e);
    Const a r_aux;
    Binop_+ r_res r_aux r_aux;
    Load r_aux r_res
  if G |- e : C (deterministic)
  and a = fieldoffset(C, f)

c_expr(e.m(e')) =
    c_expr(e);
    Push r_res;
    c_expr(e');
    Pop r_aux;
    Push r_this;
    Push r_arg;
    Mov r_aux r_this;
    Mov r_res r_arg;
    Const a r_aux;
    Jal r_aux;
    Pop r_arg;
    Pop r_this
  if G |- e : C (deterministic)
  and a = CAT(C) + methodoffset(C, m)

c_expr(e1 == e2 ? e3 : e4) =
    c_expr(e1);
    Push r_res;
    c_expr(e2);
    Pop r_aux;
    Binop_- r_aux r_res r_aux;
    Bnz r_aux k;
    c_expr(e3);
    Const 1 r_aux;                        } ugly relative jump
    Bnz k' r_aux;                         } ^^^^^^^^^^^^^^^^^^
    c_expr(e4)
  if k = |c_expr(e3)| + 2
  and k' = |c_expr(e4)|
